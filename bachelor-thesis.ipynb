{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\nimport pandas as pd\npd.options.mode.chained_assignment = None\nfrom pandas import DataFrame\n#import pytorch_multibert_embeddings\n# Reads Dataframe\nexsp = pd.read_csv('../input/extreme-speech/data-export-2021-11-03.csv')\n\nlabels = exsp.Label\n\ntarget_p = exsp.get(\"Target (protected)\")\ntarget_o= exsp.get(\"Target (others)\")\n\n# Returns list of all protected targets\ndef get_target_protected():\n    targets_protected = []\n    for line in target_p:\n        targets = str(line).split(\",\")\n        for target in targets:\n            target = target.strip()\n            if target not in targets_protected and target != \"nan\":\n                targets_protected.append(target)\n    return targets_protected\n\n# Returns list of target (others)\ndef get_target_others():\n    targets_others = []\n    for line in target_o:\n        targets = str(line).split(\",\")\n        for target in targets:\n            target = target.strip()\n            if target not in targets_others and target != \"nan\":\n                targets_o.append(target)\n    return targets_others\n\n# Returns list without NAs and duplicates\ndef unique_non_null(s):\n    return s.dropna().unique()\n\n# Returns label distribution\ndef get_label_distribution():\n    return labels.value_counts()\n\ntarget_dict = {}\n\n# Generates a dictionary that contains countries as keys and the amount of each protected target as values\nfor country in unique_non_null(exsp.Country):\n    target_dict[country] = []\n    for target in get_target_protected():\n        target_dict[country].append(sum((exsp[\"Target (protected)\"].str.contains(target)) & (exsp[\"Country\"] == country)))\n\n# Generates a new dataframe which displays the distribution of each protected target across the countries\ntarget_df = DataFrame.from_dict(\n    target_dict,\n    orient = 'index',\n    columns = get_target_protected() \n)\n\ntarget_df = target_df.transpose()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:01:12.626466Z","iopub.execute_input":"2022-06-04T20:01:12.627231Z","iopub.status.idle":"2022-06-04T20:01:22.871539Z","shell.execute_reply.started":"2022-06-04T20:01:12.627192Z","shell.execute_reply":"2022-06-04T20:01:22.870388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport nltk\n\nseed = 123\nfrom time import time\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nimport re, random, string\nfrom nltk import word_tokenize, sent_tokenize, pos_tag\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom sklearn.feature_extraction import text\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\n#from spellchecker import SpellChecker\n\nexsp_kenya = exsp[exsp['Country'] == \"Kenya\"]\n\nexsp_kenya['Label'].value_counts()\n\n# Preprocessing\ndef remove_numbers(text):\n    result = re.sub(r'\\d+', '', text)\n    return result\n\ndef remove_punctuation(text):\n    result = \"\".join([i for i in text if i not in string.punctuation])\n    return result\n\ndef remove_whitespace(text):\n    return \" \".join(text.split())\n\ndef remove_newlines(text):\n    return re.sub(r'\\r+|\\n+|\\t+','', text)\n\ndef clean_text(text):\n    text = text.lower()\n    remove_numbers(text)\n    remove_punctuation(text)\n    remove_whitespace(text)\n    text = nltk.WordPunctTokenizer().tokenize(text)\n    return text\n\n\nexsp_kenya['Text_Cleaned'] = exsp_kenya.Text.apply(lambda x:remove_punctuation(x))\nexsp_kenya['Text_Cleaned'] = exsp_kenya['Text_Cleaned'].apply(lambda x:x.lower())\nexsp_kenya['Text_Cleaned'] = exsp_kenya['Text_Cleaned'].apply(lambda x:remove_numbers(x))\nexsp_kenya['Text_Cleaned'] = exsp_kenya['Text_Cleaned'].apply(lambda x:remove_newlines(x))\n\nexsp_kenya.head()\n\nexsp_kenya[exsp_kenya.ID ==6909][\"Text\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:06:55.176657Z","iopub.execute_input":"2022-06-04T20:06:55.177494Z","iopub.status.idle":"2022-06-04T20:06:55.412095Z","shell.execute_reply.started":"2022-06-04T20:06:55.177442Z","shell.execute_reply":"2022-06-04T20:06:55.411385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\n\nvectorizer = CountVectorizer(ngram_range=(1,1), stop_words = 'english')\nx = vectorizer.fit_transform(exsp_kenya.Text_Cleaned)\n#tfidftransformer = TfidfTransformer()\n#x_tfidf = tfidftransformer.fit_transform(x)\nfeatures = vectorizer.get_feature_names_out()\nfeatures = features.tolist()\n\nprint(type(x.A))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:01:24.466652Z","iopub.execute_input":"2022-06-04T20:01:24.467463Z","iopub.status.idle":"2022-06-04T20:01:24.79031Z","shell.execute_reply.started":"2022-06-04T20:01:24.467413Z","shell.execute_reply":"2022-06-04T20:01:24.78927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids = exsp_kenya.ID\nfeature_df = pd.DataFrame(x_.A, columns = features)\nids = ids.to_numpy()\n\nfeature_df.insert(0, \"ID\", ids)\n\nlabels = exsp_kenya.Label\n\nlabel_df = pd.DataFrame(list(zip(ids, labels)), columns = [\"ID\", \"Labels\"])\nlabel_df = label_df.replace([\"dangerous speech\", \"derogatory extreme speech\", \"exclusionary extreme speech\"], [0,1,2])\nmod_df = pd.DataFrame(list(zip(ids, labels)), columns = [\"ID\", \"Moderation\"])\nmod_df = mod_df.replace([\"dangerous speech\", \"derogatory extreme speech\", \"exclusionary extreme speech\"], ['R','M','R'])\n\n\nfeature_labels_df = pd.merge(feature_df, label_df, left_on=\"ID\", right_on=\"ID\", how=\"left\")\n#print(feature_labels_df.head())\n\nfeature_mod_df = pd.merge(feature_df, mod_df, left_on=\"ID\", right_on=\"ID\", how=\"left\")\nprint(feature_mod_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:02:13.77912Z","iopub.execute_input":"2022-06-04T20:02:13.77939Z","iopub.status.idle":"2022-06-04T20:02:15.478257Z","shell.execute_reply.started":"2022-06-04T20:02:13.779359Z","shell.execute_reply":"2022-06-04T20:02:15.47747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_l = feature_labels_df.iloc[:, 1:-3]\ny_l = feature_labels_df.iloc[:, -1]\nprint(y_l)\n\nX_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.2, random_state = seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:02:23.748844Z","iopub.execute_input":"2022-06-04T20:02:23.749565Z","iopub.status.idle":"2022-06-04T20:02:24.352648Z","shell.execute_reply.started":"2022-06-04T20:02:23.749512Z","shell.execute_reply":"2022-06-04T20:02:24.351573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_m = feature_mod_df.iloc[:, 1:-3]\ny_m = feature_mod_df.iloc[:, -1]\nprint(y_m)\n\nX_m_train, X_m_test, y_m_train, y_m_test = train_test_split(X_m, y_m, test_size=0.2, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:02:31.251454Z","iopub.execute_input":"2022-06-04T20:02:31.252209Z","iopub.status.idle":"2022-06-04T20:02:31.840764Z","shell.execute_reply.started":"2022-06-04T20:02:31.25217Z","shell.execute_reply":"2022-06-04T20:02:31.840043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_id_from_index(index):\n    return feature_labels_df.iloc[index]['ID']\n    \n    \nloglabel = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear')\nloglabel.fit(X_l_train, y_l_train)\n\nlogmod = linear_model.LogisticRegression()\nlogmod.fit(X_m_train, y_m_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:02:36.78002Z","iopub.execute_input":"2022-06-04T20:02:36.780277Z","iopub.status.idle":"2022-06-04T20:02:47.123454Z","shell.execute_reply.started":"2022-06-04T20:02:36.780249Z","shell.execute_reply":"2022-06-04T20:02:47.122398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print('Predicted value is=', lm.predict(X_test))\n#print('Actual value from test data is ', y_test)\n#type(y_test)\n\ntest_index = list(X_l_test.index.values)\ntest_ids = []\nfor test_idx in test_index:\n    test_ids.append(get_id_from_index(test_idx))\ntest_ids = np.array(test_ids)\n    \ntest_text = []\nfor test_id in test_ids:\n    test_text.append(exsp_kenya[exsp_kenya['ID']==test_id][\"Text\"])\ntest_text = np.array(test_text).flatten()\n#rint(test_text)\n    \ntest_label = []\nfor test_id in test_ids:\n    test_label.append(exsp_kenya[exsp_kenya['ID'] ==test_id][\"Label\"])\ntest_label = np.array(test_label).flatten()\n\ntest_mod = []\nfor test_id in test_ids:\n    test_mod.append(feature_mod_df[feature_mod_df['ID']==test_id][\"Moderation\"])\ntest_mod = np.array(test_mod).flatten()\n\nlabel_test_predictions = loglabel.predict(X_l_test)\nlabel_test_predictions = label_test_predictions.astype('str')\n\nlabel_test_predictions[label_test_predictions == '0'] = \"dangerous speech\"\nlabel_test_predictions[label_test_predictions == '1'] = \"derogatory extreme speech\"\nlabel_test_predictions[label_test_predictions == '2'] = \"exclusionary extreme speech\"\n#pint(test_predictions)\n\nmod_test_predictions = logmod.predict(X_m_test)\nmod_test_predictions = mod_test_predictions.astype('str')","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:01:40.189049Z","iopub.status.idle":"2022-06-04T20:01:40.189927Z","shell.execute_reply.started":"2022-06-04T20:01:40.189672Z","shell.execute_reply":"2022-06-04T20:01:40.189709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_results = np.array([test_ids, test_text, test_label, label_test_predictions, test_mod, mod_test_predictions])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:01:40.190922Z","iopub.status.idle":"2022-06-04T20:01:40.191724Z","shell.execute_reply.started":"2022-06-04T20:01:40.19147Z","shell.execute_reply":"2022-06-04T20:01:40.191495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nresults = pd.DataFrame(data=test_results)\nresults = results.T\nresults.columns = [\"ID\", \"Text\", \"Actual label\", \"Predicted Label\", \"Moderation\", \"Predicted Moderation\"]\n\nresults.to_csv('results.csv', index=False)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:01:40.192796Z","iopub.status.idle":"2022-06-04T20:01:40.193565Z","shell.execute_reply.started":"2022-06-04T20:01:40.193299Z","shell.execute_reply":"2022-06-04T20:01:40.193326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# naive bayes\nnbclasslabel = MultinomialNB(alpha=0.1).fit(X_l_train, y_l_train)\nvect = CountVectorizer().fit(X_l_train)\npredict = nbclasslabel.predict(vect.transform(X_l_test))\nfor el in predict:\n    print(el)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Activation, Dropout\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:42:02.800976Z","iopub.execute_input":"2022-06-04T20:42:02.801228Z","iopub.status.idle":"2022-06-04T20:42:02.805648Z","shell.execute_reply.started":"2022-06-04T20:42:02.801201Z","shell.execute_reply":"2022-06-04T20:42:02.804973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_words, batch_size, epochs = 10000, 256, 3\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(X_l_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(X_l_test, mode='binary')\n\ny_train = keras.utils.to_categorically(y_l_train, 3)\ny_test = keras.utils.to_categorically(y_l_test, 3)\n\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:50:33.066311Z","iopub.execute_input":"2022-06-04T20:50:33.066623Z","iopub.status.idle":"2022-06-04T20:50:33.09641Z","shell.execute_reply.started":"2022-06-04T20:50:33.066589Z","shell.execute_reply":"2022-06-04T20:50:33.095473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}